\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts, amsmath, enumitem, float, graphicx, hyperref}

\title{Numerical Computing - Notes}
\author{Matteo Alberici}
\date{January 2022}

\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage

% --------------------------------- %
% Assignment 1 - PageRank Algorithm
% --------------------------------- %
\section{PageRank Algorithm}
\subsection{Eigenvectors}
An \textbf{eigenvector} $v \in \mathbb{R}$ of a matrix $A$ is a nonzero vector such that the relation
\begin{center}
    $Av = \lambda v$ 
\end{center}
holds, where $\lambda$ is the \textbf{eigenvalue} associated with vector $v$. \\
An \textbf{eigenbasis} is a basis in which every vector is an eigenvector. \\
Given a vector $v$, the \textbf{Rayleigh quotient} computes the eigenvalue of $v$ whether $v$ is an eigenvector, or the eigenvalue associated with closest eigenvector of $v$ otherwise:
\begin{center}
    $\mu(v) = \displaystyle\frac{v^TAv}{v^Tv}$
\end{center}
\subsection{PageRank Algorithm}
The \textbf{PageRank} is determined by the structure of the World Wide Web. For any query, Google lists the matching Web pages in their PageRank order: a Web page has a high rank if other pages with a high rank link to it. \\
The algorithm is based on the \textbf{Random Surfer Model}, in which a user goes from one page to another by randomly choosing an outgoing link through \textbf{exploitation}. Since surfing randomly could lead to dead ends or cycles of Web pages, a random Web page is chosen through \textbf{exploration}. The random walk generated by exploitation and exploration is known as the \textbf{Markov chain}. \\ 
Let's introduce some definitions for the PageRank computation:
\begin{itemize}
    \item $n \ = \ $ number of Web pages
    \item $G \ = \ $ $n-$by$-n$ connectivity matrix of a Web portion with:
        \begin{center}
            $g_{ij} = 
            \begin{cases}
                1 & \text{hyperlink from page } j \text{ to page } i  \\
                0 & \text{otherwise}
            \end{cases}$
        \end{center}
    \item $r_i \ = \ $ row sum of $G$ and in-degree of $j-$th page:
        \begin{center}
            $r_i = \displaystyle\sum_j g_{ij}$
        \end{center}
    \item $c_j \ = \ $ column sum of $G$ and out-degree of $j-$th page:
        \begin{center}
            $c_j = \displaystyle\sum_i g_{ij}$
        \end{center}
    \newpage
    \item $p \ = \ $ probability that the random walk follows a link
        \begin{itemize}
            \item the typical value is: $\ p = 0.85$
            \item the probability that some arbitrary page is chosen is: $\ 1 - p$
            \item the probability that a particular random page is chosen is:
                \begin{center}
                    $\delta = \displaystyle\frac{1 - p}{n}$
                \end{center}
        \end{itemize}
    \item $A \ = \ $ $n-$by$-n$ transition probability matrix of the Markov chain with:
        \begin{center}
            $a_{ij} = 
            \begin{cases}
                p \cdot \frac{g_{ij}}{c_j} + \delta & c_j \neq 0 \\
                \frac{1}{n} & c_j = 0
            \end{cases}$
        \end{center}
\end{itemize}
All the elements in matrix $A$ are strictly between $0$ and $1$ and all the column sums are equal to $1$. \\
The \textbf{Perron-Frobenius theorem} states that a nonzero solution of the equation
\begin{center}
    $x = Ax$
\end{center}
exists and is unique to within a scaling factor. If this factor is chosen so that:
\begin{center}
    $\displaystyle\sum_i x_i = 1$,
\end{center}
then $x$ is the \textbf{state vector} of the Markov chain and Google's PageRank. Its elements are strictly between $0$ and $1$ and it is the solution to the following linear system:
\begin{center}
    $(I - A)x = 0$
\end{center}
The best way to compute PageRank in MATLAB is defining matrix $A$ as follows:
\begin{center}
    $A = p GD + ez^T$, where:
\end{center}
\begin{itemize}
    \item $D\ = \ $ diagonal matrix formed from the out-degrees reciprocals:
        \begin{center}
            $d_{jj} = 
            \begin{cases}
                1/ c_j & c_j \neq 0 \\
                0 & c_j = 0
            \end{cases}$
        \end{center}
    \item $e \ = \ $ $n-$vector of all ones
    \item $z \ = \ $ vector with:
        \begin{center}
            $z_j = 
            \begin{cases}
                \delta & c_j \neq 0 \\
                1/n & c_j = 0
            \end{cases}$
        \end{center}
\end{itemize}
The assignment statement can now be written as follows:
\begin{center}
    $(I - pGD)x = \lambda e$,
\end{center}
where $ \ \lambda = z^Tx \ $ has the temporary value of $1$.
\subsection{Power Method}
The \textbf{power method} is used to compute the dominant eigenvector $\lambda_1$ of a matrix $A$:
\begin{enumerate}
    \item Start with an initial guess vector $v_0$:
    \item Compute $w$ in the following way:
    \begin{center}
        $w = Av_k$
    \end{center}
    \item Set $v_{k+1} = ||w||$
    \item Find the eigenvalue of $v_{k+1}$ by using the Rayleigh quotient
    \item Repeat from step $2$ incrementing $k$ by $1$ until the difference between the previous and the current eigenvectors is lower than a certain threshold
\end{enumerate}
In order to guarantee convergence, we assume that matrix $A$ has an eigenvalue $\lambda$ whose magnitude is strictly greater than the others and that vector $x$ has a nonzero component in the direction of the eigenvector associated with $\lambda$.
\subsection{Shift and Invert Method}
Since the power method converges linearly and its error constant is $|\lambda_2 / \lambda_1|$, the convergence could be too slow if $||\lambda_2||$ and $||\lambda_1||$ are close: the \textbf{shift and invert method} is used to improve the power method's converging rate. \\
Given a matrix $A$, we define matrix $B$ as:
\begin{center}
    $B = (A - \alpha I)^{-1}$
\end{center}
The eigenvalues of $B$ are defined as:
\begin{center}
    $u_j = \displaystyle\frac{1}{\lambda_j - \alpha}$.
\end{center}
By applying the power method to matrix $B$ and assuming $\lambda_2$ is the closest-to-$\lambda_1$ eigenvalue of $A$, we obtain the following improved converging rate:
\begin{center}
    $|\displaystyle\frac{u_2}{u_1}| = |\displaystyle\frac{\lambda_1 - \alpha}{\lambda_2 - \alpha}|$
\end{center}
The cost of a power method iteration is given by a vector-matrix product, the cost of an iteration in the shift and invert method is given by a linear system.

\newpage

% ------------------------------ %
% Assignment 2 - Social Networks
% ------------------------------ %
\section{Social Networks}
\subsection{Cholesky Factorization}
The \textbf{Cholesky factorization} is the decomposition of a symmetric, positive-definite matrix $A$ into the product of its lower triangular form $L$ and its conjugate transpose $L^T$:
\begin{center}
    $A = LL^T$
\end{center}
It is twice as efficient as the LU decomposition. 
\subsection{Reverse Cuthill McKee Ordering}
The \textbf{reverse Cuthill McKee ordering} is the permutation of a symmetric sparse matrix into a banded matrix with a smaller bandwidth. The resulting matrix has the non-zero elements closer to the diagonal. \\
In MATLAB, we can use the following implementation:
\begin{verbatim}
    1) r = symrcm(A(2:end, 2:end));
    2) prcm = [1 r+1];
\end{verbatim}
This method is useful to reduce the number of fill-ins, making less expensive the Cholesky factorization.
\subsection{Spectral Graph Partitioning}
In \textbf{spectral graph partitioning}, we plant a random partition assigning values to the probabilities of intra-sets and inter-sets edges, then use the \textbf{Fiedler's vector}, which is the eigenvector $v_2$ associated with the second smallest eigenvalue $\lambda_2$, to find the partition. \\
In the Fiedler's vector, all indices of entries $ > 0$ belong to one set and those of entries $< 0$ belong to the other. The partition minimizes the number of edges between the sets.
\subsection{Degree Centrality}
The \textbf{degree centrality} consists of ranking the number of incident links upon a node. For a given graph $G = (V,E)$ it is defined as the number of edges of a vertex $v$. \\
The \textbf{eigenvector centrality} measures the influence of a node in a network. The relationships between nodes with a high score contribute more to the node score.

% --------------------------------- %
% Assignment 3 - Graph Partitioning
% --------------------------------- %
\section{Graph Partitioning}
\subsection{Graphs Matrices}
\subsubsection{Degree Matrix}
A \textbf{degree matrix} $D$ is a diagonal matrix containing the degree of each vertex. If the graph is weighted, the entries are the sum of all the weights of the connected edges.
\subsubsection{Adjacency Matrix}
An \textbf{adjacency matrix} is a square matrix representing the connections of an unweighted graph and its entries are defined as follows:
\begin{center}
    $a_{ij} = 
    \begin{cases}
        1 & v_i \text{ connected to } v_j \\
        0 & \text{otherwise}
    \end{cases}$
\end{center}
\subsubsection{Weight Matrix}
A \textbf{weight matrix} $W$ is a square matrix representing the connections of a weighted graph and its entries are defined as follows:
\begin{center}
    $w_{ij} = 
    \begin{cases}
        w & v_i \text{ connected to } v_j \\
        0 & \text{otherwise}
    \end{cases}$
\end{center}
\subsubsection{Laplacian Matrix}
The \textbf{Laplacian matrix} $L$ is the matrix representation of a graph and is computed in the following ways:
\begin{itemize}
    \item For an undirected and unweighted graph
        \begin{center}
            $L = D - A $
        \end{center}
    \item For an undirected and weighted graph
        \begin{center}
            $L = D - W $
        \end{center}
\end{itemize}
Matrix $L$ only has real and non-negative eigenvalues and its eigenvectors are real and orthogonal.
\subsection{Graph Partitioning}
The \textbf{graph partitioning} problem is defined on a graph $G = (V, E) $ such that it is possible to partition $G$ into smaller components with specific properties, generally with small cuts and equal-size partitions.
\subsection{Spectral Bisection}
\textbf{Spectral bisection} enables the decomposition of a symmetric matrix into eigenvalues within an orthonormal base of eigenvectors. The following procedure performs a spectral bisection:
\begin{enumerate}
    \item Compute the Laplacian matrix $L$
    \item Compute the Fiedler's vector $w_2$
    \item Set a threshold to $0$ or to the median of $w_2$
    \item Choose $V_1 = \{v_i \in V | w_i < \text{threshold}\}$ and $V_2 = \{v_i \in V | w_i \geq \text{threshold}\}$
    \item Return $V_1$ and $V_2$
\end{enumerate}
Thresholding the values of $w_2$ around $0$ results in two roughly equal-sized partitions with minimum edgecut, while thresholding around the median value of $w_2$ produces two strictly balanced partitions.
\subsection{Inertial Bisection}
\textbf{Inertial bisection} relies on the vertices' geometric coordinates: it finds the hyperplane running through the center of mass of the points. \\
In $2D$, the line is chosen such that the sum of squares of the distances of the nodes to the line is minimized. \\
The following procedure performs an inertial bisection:
\begin{enumerate}
    \item Compute the center of mass of the points as follows:
        \begin{center}
            $x = \displaystyle\frac{1}{n} \displaystyle\sum_{i = 1}^n x_i \ \ \ \ \ y = \displaystyle\frac{1}{n}\displaystyle\sum_{i = 1}^n y_i$
        \end{center}
    \item Compute matrix $M$ as follows:
        \begin{center}
            $S_{xx} = \displaystyle\sum_{i = 1}^n(x_i - x)^2 \ \ \ \ S_{yy} = \displaystyle\sum_{i = 1}^n(y_i - y)^2 \ \ \ \ S_{xy} = \displaystyle\sum_{i = 1}^n((x_i - x)(y_i - y))$ \\
            \vspace{0.5cm}
            $ M = 
            \begin{bmatrix}
                S_{xx} & S_{xy} \\
                S_{xy} & S_{yy}
            \end{bmatrix}$
        \end{center}
    \item Compute the smallest eigenvalue of matrix $M$ and the associated eigenvector $u$ \item Minimize the distance of the nodes to the line:
        \begin{center}
            $u^TMu$
        \end{center}
    \item Project each point to the line and compute the median to partition the nodes
    \item Return $V_1$ and $V_2$
\end{enumerate}
\subsection{Recursive Bisection}
\textbf{Recursive bisection} depends on the decisions made during the earlier stages of the process and lacks of global information, meaning that it may result in suboptimal partitions.
\subsection{$K-$Way Partitioning}
\textbf{$K-$way partition} starts with partitioning a small set of vertices, then projects it back towards the original set to refine it. The main difference with recursive bisection is that $k-$way partitioning stores global information about the graph.
\subsection{Partitioning Metrics}
The number of cut edges between partitions determines the result's quality. The size of an \textbf{edgecut} partitioning the graph into two complementary vertices subsets is defined as follows:
\begin{center}
    $cut(V_1,V_2) = \displaystyle\sum_{i \in V_1, j \in V_2} w_{ij}$
\end{center}
The \textbf{cardinality}, i.e. the number of nodes, of each subset is given by:
\begin{center}
    $x^Tx = ||x||_2^2 = |V_1|$
\end{center}
Let $\Tilde{V}$ be the average partition weight, then for a $k-$way partition the \textbf{load imbalance} $b^k$ is defined as follows:
\begin{center}
    $b^k = max_{|V_i^k|} \ \ \displaystyle\frac{|V_i^k|}{|\Tilde{V}^k|}$ \\
    \vspace{0.3cm}
    $b^k = 1 + b^k_r \geq 1$ \\
    \vspace{0.35cm}
    $b^k_r \geq 0 $
\end{center}
It characterizes the deviation from obtaining a balanced partitioning. \\
The optimal value for $b^k_r$ is $0$ and implies that the $k$ partitions contain the same number of nodes.

\newpage

% ---------------------------------------- %
% Assignment 4 - Spectral Graph Clustering
% ---------------------------------------- %
\section{Spectral Graph Clustering}
\subsection{Graph Clustering}
Given a set of data points $x_1,...,x_n$ and some notion of similarity $s_{ij} \geq 0$, clustering extracts information by grouping similar data points.
\subsection{Trees}
A \textbf{tree} is an undirected graph in which any two vertices are connected by exactly one path. A \textbf{minimum spanning tree} is a subgraph including all the vertices of the graph, the minimum possible number of edges and the minimum possible total weight.
\subsection{Similarity Graphs}
In \textbf{similarity graphs} two vertices $v_i$ and $v_j$ are connected based on the similarity $s_{ij}$ between the corresponding data points $x_i$ and $x_j$. The edge between them is weighted by $s_{ij}$. \\
The goal is to model the neighborhood relationships between the data points.
\subsubsection{$\epsilon-$Neighborhood Graph}
In $\epsilon-$neighborhood graphs, we connect two vertices if their pairwise distances are smaller than $\epsilon$. Weighting the edges would not incorporate more information about the data to the graph, thus they are unweighted.
\subsubsection{$k-$Nearest Neighbor Graphs}
In \textbf{$k-$nearest neighbor graphs}, we connect vertices $v_i$ and $v_j$ if the latter is among the $k-$nearest neighbors of the first. The graph is directed and we have two ways to make it undirected: either we ignore the directions obtaining a simple $k-$nearest neighbors graph, or we connect the vertices if each of them is among the $k-$nearest neighbors of the other, obtaining a \textbf{mutual $k-$nearest neighbors graph}.
\subsubsection{Fully Connected Graph}
In \textbf{fully connected graphs}, we connect two vertices if the similarity $s_{ij} > 0$. This construction is useful only if the similarity function models local neighborhoods.
\newpage
\subsection{Spectral Clustering}
In \textbf{spectral clustering}, we create clusters with the same number of nodes through the following procedure:
\begin{enumerate}
    \item Compute the minimum spanning tree to determine $\epsilon$
    \item Create the $\epsilon-$neighbourhood graph
    \item Compute the adjacency matrix $W$ as follows:
    \begin{center}
        $W = S \odot G$
    \end{center}
    \item Compute the Laplacian matrix
    \item Compute the $k-$smallest eigenvectors based on $L$
    \item Use k-means algorithm to cluster the nodes
    \item Use the eigenvector matrix in order to obtain the clusters
\end{enumerate}
\subsection{K-Means Clustering}
The \textbf{k-means clustering} divides the clusters based on the distance between a node and a \textbf{centroid}, which represents the center of a cluster. The algorithm is the following:
\begin{enumerate}
    \item Start with some random centroids
    \item Assign each node to the nearest cluster based on its centroid
    \item Recompute the centroids as the mean of the points in the cluster
    \item Repeat from step $2$ until no further improvement can be made
\end{enumerate}

\newpage

% ------------------------------------------------------ %
% Assignment 5 - Image Deblurring and Conjugate Gradient
% ------------------------------------------------------ %
\section{Image Deblurring and Conjugate Gradient}
\subsection{Blurred Image Problem Definition}
Let matrix $B \in \mathbb{R}^{n^2 \times n^2}$ be a blurred image we want to deblur given the transformation and let $X \in \mathbb{R}^{n \times n}$ be the original square and greyscale image matrix where each entry corresponds to a pixel value. We can perform \textbf{vectorization} by converting matrix $X$ into a column vector $x \in \mathbb{R}^{n^2}$ and matrix $B$ into vector $b \in \mathbb{R^{n^2}}$. \\
We can write the following equation:
\begin{center}
    $Ax = b$,
\end{center}
where $A \in \mathbb{R}^{n^2 \times n^2}$ indicates the transformation matrix coming from the repeated application of the image kernel. A blurred pixel is the weighted average of the surrounding ones, with the weights defined by the kernel matrix $K \in \mathbb{R}^{d\times d}$. The non-zero elements of each row of matrix $A$ correspond to the values of matrix $K$.
Matrix $A$ ignores the elements outside the matrix borders:
\begin{center}
    $(max(i, j) > n)$
\end{center}
Matrix $A$ is a $d^2-$banded matrix, where:
\begin{center}
    $d << n$,
\end{center}
meaning that matrix $A$ is sparse.
\subsection{Direct and Iterative Methods}
Since the complexity of Gaussian elimination for large linear systems is too high, we need another approach: we use \textbf{direct methods} to compute the exact solution in $n$ steps, while we use \textbf{iterative methods} with an arbitrary starting point to compute an approximate solution. Iterative methods converge after a few iterations, but they are less robust and not as general as direct ones.
\subsection{Error and Residual}
Let $x$ be an exact solution and $x^m$ be the computed point, then the \textbf{error} in step $m$ is the deviation of the computed point from the exact solution:
\begin{center}
    $e^{m} = x - x^m$,
\end{center}
The error is not known during the iterations, otherwise we would know the solution. \\
The \textbf{residual} provides us with a measure of the real error:
\begin{center}
    $r^m = b - Ax^m$
\end{center}
\newpage
\subsection{Steepest Descent}
The \textbf{steepest descent} algorithm is a precursor to the Conjugate Gradient algorithm that performs the following procedure:
\begin{enumerate}
    \item Start with a random initial guess
    \item Take the \textbf{gradient}, i.e. the direction of the deepest descent
    \item Compute the minimum of the gradient
    \item Take the new gradient such that it is orthogonal to the previous one
    \item Repeat from step $2$ until a certain convergence criterion is met
\end{enumerate}
This algorithm has a low convergence rate, but it does not take optimal steps to find the next approximate solution.
\subsection{Conjugate Gradient}
The \textbf{conjugate gradient method} is used to solve equations of the type:
\begin{center}
    $Ax = b$,
\end{center}
where $A$ is symmetric and positive-definite. It can be used as an iterative method since it provides monotonically improving approximations $x_k$ to the exact solution after each iteration, with a number of iterations not larger than the size of the matrix, in the absence of round-off error. The improvement is typically linear and its speed is determined by the \textbf{condition number} $\kappa(A)$ of the matrix $A$: 
\begin{center}
    $\kappa(A) = \displaystyle\frac{\sigma_{max}}{\sigma_{min}}$,
\end{center}
with $\sigma$ indicating the singular values of $A$. For a real symmetric matrix, the values are equal to the eigenvalues absolute value:
\begin{center}
    $\sigma = |\lambda|$ \\
\end{center}
The larger the condition number is, the slower the improvement.
\newpage
The algorithm starts from an initial guess $x_0$ and applies a series of operations until the desired tolerance is reached:
\begin{enumerate}
    \item $r = b - Ax_0$
    \item $d = r$
    \item $\rho_{old} = <r,r>$
    \item $for \ i = 0,1,... \ do:$
    \begin{enumerate}[label=\arabic*.]
        \item $ s = A \cdot d_i$
        \item $\alpha = \rho_{old} / <d,s>$
        \item $x = x + \alpha \cdot d$
        \item $r = r - \alpha \cdot s$
        \item $\rho_{new} = <r,r>$
        \item $\beta = \rho_{new} / \rho_{old}$
        \item $d = r + \beta \cdot d$
        \item $\rho_{old} = \rho_{new}$
    \end{enumerate} 
    \item $end \ for$
\end{enumerate}
\vspace{0.2cm}
Since matrix $A$ is not positive-definite we must solve the following equations:
\begin{center}
    $A^TAx = A^Tb \ \rightarrow \ \Tilde{A}x = \Tilde{b}$
\end{center}
The pre-multiplication with $A^T$ results in the positive-definite augmented transformation matrix $\Tilde{A}$. \\
The condition number is the relation of the sensitivity of the solution $x$ to changes in $b$: if small changes result in large changes in $x$, then the system is \textbf{ill-conditioned} and its condition number is large. The convergence rate of the algorithm is hindered when the system has a high condition number.
\subsection{Preconditioned Conjugate Gradient}
In the \textbf{preconditioned conjugate gradient method} (PCG), a symmetric and positive-definite \textbf{preconditioner} $P$ is selected such that:
\begin{center}
    $P^{-1} \Tilde{A} \approx I$
\end{center}
Furthermore, we can decompose the preconditioner such that:
\begin{center}
    $P = LL^T$,
\end{center}
where $L$ is the Cholesky factor.
\newpage
Now we have to solve the preconditioned augmented system:
\begin{center}
    $P^{-1}\Tilde{A}x = P^{-1}\Tilde{b}$ \\
    \vspace{0.25cm}
    $(L^{-1}\Tilde{A}L^{-1})(Lx) = L^{-1}\Tilde{b}$
\end{center}
This is done to decrease the condition number and the range of the eigenvalues. The preconditioner should be computationally inexpensive to find.
We can use the \textbf{incomplete Cholesky factorization} to compute the Cholesky factorization of the non-zero elements of $\Tilde{A}$, returning the following preconditioner: 
\begin{center}
    $P = F^TF$,
\end{center}
where $F$ is the sparse incomplete Cholesky factor. Due to the fact that the routine fail since the existence of $F$ is not guaranteed, a heuristic approach is used to apply a diagonal shift of $P$, enforcing positive-definiteness and making $F$ computable. \\
PCG would be worth the added computational cost whether the conditional number $\kappa(A)$ is large, i.e. matrix $A$ is ill-conditioned since with CG it will require a lot of iterations. While deblurring lots of images with PCG, the computational cost would decrease since it is possible to use the same precondition in order to save time.

\newpage

% -------------------------------------------------------- %
% Assignment 6 - Linear Programming and the Simplex Method
% -------------------------------------------------------- %
\section{Linear Programming and the Simplex Method}
\subsection{Linear Programming}
\textbf{Linear Programming} is an optimization technique used to either maximize or minimize a linear objective function subject to linear equality and inequality constraints. An example of a linear program is the following:
\begin{align*}
	\max~~ & \sum_{i = 1}^n c_ix_i \\
	\text{s.t.}~~ & \sum_{j = 1}^n a_{1,j}x_j \leq h_1 \\
	~~&~~ \vdots \\
	~~& \sum_{j = 1}^n a_{m,j}x_j \leq h_m
\end{align*}
Vector $x_i$ must satisfy some constraints and the non-negativity condition. \\
Linear programming problems can be written in the following \textbf{standard form}:
\begin{equation*}
  \begin{split}
    \max~~ & z = c^Tx \\
	\text{s.t.}~~ & Ax \leq h \\
	~~& x \geq 0
  \end{split}
\quad\quad\quad\quad
  \begin{split}
    \min~~ & z = c^Tx \\
	\text{s.t.}~~ & Ax \geq h \\
	~~& x \geq 0
  \end{split}
\end{equation*} \\
where $z $ is the value of the objective function, $c \in \mathbb{R}^n$ is the coefficients vector, $x \in \mathbb{R}^n$ is the unknowns vector, $A \in \mathbb{R}^{m \times n}$ is the coefficients matrix, and $h \in \mathbb{R}^m$ is the vector of the constraints coefficients. \\
Given that the \textbf{feasible region} is generated by the vertices that satisfy the constraints, the optimal value can be found with the \textbf{Fundamental Theorem of Linear Programming}: \\ \\
\textbf{Theorem 1} \textit{If a linear program admits a solution, it will lie on a vertex of the polytope defined by the feasible region. If two vertices are both maximizers or minimizers of the function, then all the points lying on the segment between them will represent the optimal solutions to the problem.}
\subsection{The Simplex Method}
Since computing the value of $z$ at all the vertices of the feasible region could be expensive, we use the \textbf{simplex method}, which has an exponential worst-case complexity.
\newpage
\subsubsection{Slack and Surplus Variables}
The method introduces \textbf{slack variables} for maximization and \textbf{surplus variables} for minimization to the standard form of the problem, denoting them with $s_m$ as in the following example:
\begin{equation*}
  \begin{split}
  	\max~~ & z = 3x+2y \\
	\text{s.t.}~~ & x+2y+s_1 = 4 \\
	~~& x-y+s_2 = 1 \\
	~~& x,y \geq 0; \ ~ s_1, s_2 \geq 0
  \end{split}
\quad\quad\quad\quad
  \begin{split}
    \min~~ & z = 3x+2y \\
	\text{s.t.}~~ & x+2y-s_1 = 4 \\
	~~& x-y-s_2 = 1 \\
	~~& x,y \geq 0;~ \ s_1, s_2 \geq 0
  \end{split}
\end{equation*}
\subsubsection{Basic and Nonbasic Variables}
We can swap the rows of matrix $A$ as long as we swap the elements of vector $h$. Furthermore, we can swap the columns of matrix $A$ as long as we swap the elements of vector $x$. Matrix $A$ can be split in two submatrices:
\begin{center}
    $A = [B \ D]$,
\end{center}
where matrix $B$ contains the linearly independent columns of $A$, while matrix $D$ contains the remaining ones. We can split vectors $x$ and $c$ in the same way:
\begin{center}
    $x =
    \begin{bmatrix}
    x_B \\
    x_D
    \end{bmatrix}
    ~~~~ c =
    \begin{bmatrix}
    c_B \\
    c_D
    \end{bmatrix}$
\end{center}
Finally, we obtain the following relation:
\begin{center}
    $x_B = B^{-1}h - B^{-1}Dx_D$
\end{center}
Vector $x_B$ contains the \textbf{basic variables}, while vector $x_D$ contains the \textbf{nonbasic variables}. By setting $ \ x_D = 0 \ $, we obtain the following equation:
\begin{center}
    $x_B = B^{-1}h$,
\end{center}
and if the non-negativity condition for $x_B$ is satisfied, then this is a \textbf{basic solution} and corresponds to the feasible region. If $x_B$ has some zero values, then the solution is \textbf{degenerate}. \\
The existence of a feasible solution implies the existence of a feasible basic solution, and the existence of an optimal solution implies the existence of an optimal basic solution. \\
The number of possible basic solutions grows exponentially with unknowns and constraints. The maximum possible number of iteration $N$ is:
\begin{center}
    $N = \displaystyle\frac{(m +n)!}{m! n!}$
\end{center}
\subsubsection{Optimality Condition}
We must check if the solution satisfies the \textbf{optimality condition}, based on the basis of the \textbf{reduced cost coefficients}, at every iteration:
\begin{center}
    $r_D = c_D^T - c_B^T B^{-1}D$,
\end{center}
where $c_B$ is the basic coefficient vector, $c_D$ is the nonbasic coefficient vector, $B$ is the basic matrix, and $D$ is the nonbasic matrix. \\
The optimality condition for maximization is $r_D \leq 0$, while it is $r_D \geq 0$ for minimization. Both are satisfied when all the components of the reduced cost coefficients vectors are less or bigger than $0$, respectively.
\subsubsection{Iterative Rule}
If the optimality condition is not met, we take the variable with the highest reduced cost coefficient, in the case of maximization, or the variable with the lowest cost coefficient, in the case of minimization, into the basis. \\
If two variables have the same value, we could end up swapping them over and over again creating a \textbf{cycle}. The \textbf{iterative rule} identifies the variables that must be taken out of the basis:
\begin{center}
    $\displaystyle\frac{B^{-1}h}{B^{-1}D}$
\end{center}
We select the ratio with the smallest positive value.
\subsubsection{Simplex Method Procedure}
The method procedure can be summarized as follows:
\begin{enumerate}
    \item Write the problem in standard form
    \item Add slack or surplus variables
    \item Apply the iterative rule by exchanging basic with nonbasic variables
    \item Repeat from step $3$ until the optimality condition is met
\end{enumerate}
\subsection{The Auxiliary Problem}
We could find a feasible initial basic solution by solving an \textbf{auxiliary problem}, which can be defined by introducing the \textbf{artificial variables} $u_m$:
\begin{align*}
	\max~~ & z_{\text{aux}} = \sum_{i = 1}^n u_i \\
	\text{s.t.}~~ & \sum_{j = 1}^n a_{1,j}x_j+s_1+u_1 = h_1 \\
	~~&~~ \vdots \\
	~~& \sum_{j = 1}^n a_{m,j}x_j+s_m+u_m = h_m \\
	~~& x_i, ..., x_n \geq 0;~ s_1, ..., s_m \geq 0;~ u_1, ..., u_m \geq 0
\end{align*}
The auxiliary problem aims at minimizing the sum of artificial variables and its optimal solution would be achieved when all the artificial variables are $ = 0$. If $z_{aux} = 0$ is not achieved, then the auxiliary problem and the original problem do not admit a feasible solution. \\
In order to obtain a starting basic solution for the auxiliary problem, we can set the original and the slack variables to $0$ and the artificial variables equal to the right-hand side.

\end{document}
